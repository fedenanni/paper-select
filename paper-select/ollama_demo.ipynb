{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fnanni/Projects/paper-select/.venv/lib/python3.9/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = Ollama(model=\"llama2\", request_timeout=60.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All abstracts saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import feedparser\n",
    "import os\n",
    "\n",
    "#url = \"https://connect.biorxiv.org/biorxiv_xml.php?subject=developmental_biology\"\n",
    "\n",
    "url = \"http://arxiv.org/rss/cs.CL\"\n",
    "\n",
    "# Make a GET request to fetch the content of the page\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the content as XML using feedparser\n",
    "feed = feedparser.parse(response.content)\n",
    "\n",
    "# Create the 'abstracts' folder if it doesn't exist\n",
    "if not os.path.exists('abstracts'):\n",
    "    os.makedirs('abstracts')\n",
    "# else, remove all files in the folder\n",
    "else:\n",
    "    for file in os.listdir('abstracts'):\n",
    "        file_path = os.path.join('abstracts', file)\n",
    "        os.remove(file_path)\n",
    "\n",
    "# Access individual entries and save them as separate documents\n",
    "for i, entry in enumerate(feed.entries, start=1):\n",
    "    title = entry.title\n",
    "    summary = entry.summary\n",
    "    link = entry.link\n",
    "    # Create a file path for the abstract\n",
    "    file_path = os.path.join('abstracts', f'abstract_{i}.txt')\n",
    "\n",
    "    # Write the title and summary to the file\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(f'Title: {title}\\n\\n')\n",
    "        file.write(f'Link: {link}\\n\\n')\n",
    "        file.write(f'Summary: {summary}\\n')\n",
    "\n",
    "print(\"All abstracts saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Input documents: 149\n"
     ]
    }
   ],
   "source": [
    "# Load documents from a directory\n",
    "from llama_index.core import SimpleDirectoryReader,ServiceContext,VectorStoreIndex\n",
    "\n",
    "documents = SimpleDirectoryReader('abstracts').load_data()\n",
    "print(\"Number of Input documents:\", len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bp/v78dbzp15sjc_j85g_r66t8w0000gr/T/ipykernel_23099/2407464536.py:1: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context = ServiceContext.from_defaults(llm=Settings.llm,embed_model=\"local\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 149\n"
     ]
    }
   ],
   "source": [
    "service_context = ServiceContext.from_defaults(llm=Settings.llm,embed_model=\"local\")\n",
    "index = VectorStoreIndex.from_documents(documents,service_context=service_context)\n",
    "query_engine = index.as_query_engine(llm=Settings.llm)\n",
    "\n",
    "print(\"Number of nodes:\", len(index.docstore.docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here are the grouped papers you provided, along with their novelty scores (on a scale of 0-10):\n",
      "\n",
      "**Sparse Logistic Regression with High-order Features for Automatic Grammar Rule Extraction from Treebanks**\n",
      "\n",
      "* Title: Sparse Logistic Regression with High-order Features for Automatic Grammar Rule Extraction from Treebanks\n",
      "* Link: <https://arxiv.org/abs/2403.17534>\n",
      "* Abstract: 8/10\n",
      "\n",
      "**Large Language Modeling for Scientific Hypothesis Testing: A Survey**\n",
      "\n",
      "* Title: Large Language Modeling for Scientific Hypothesis Testing: A Survey\n",
      "* Link: <https://arxiv.org/abs/2309.06578v3>\n",
      "* Abstract: 7/10\n",
      "\n",
      "**EthioLLM: Multilingual Large Language Models for Ethiopian Languages with Task Evaluation**\n",
      "\n",
      "* Title: EthioLLM: Multilingual Large Language Models for Ethiopian Languages with Task Evaluation\n",
      "* Link: <https://arxiv.org/abs/2403.13737>\n",
      "* Abstract: 9/10\n",
      "\n",
      "**Hyacinth6B: A Large Language Model for Traditional Chinese**\n",
      "\n",
      "* Title: Hyacinth6B: A Large Language Model for Traditional Chinese\n",
      "* Link: <https://arxiv.org/abs/2403.13281>\n",
      "* Abstract: 9/10\n",
      "\n",
      "**Mastering Text, Code and Math Simultaneously via Fusing Highly Specialized Language Models**\n",
      "\n",
      "* Title: Mastering Text, Code and Math Simultaneously via Fusing Highly Specialized Language Models\n",
      "* Link: <https://arxiv.org/abs/2403.17491>\n",
      "* Abstract: 9/10\n",
      "\n",
      "I hope this helps! Let me know if you have any other questions."
     ]
    }
   ],
   "source": [
    "# Setup index query engine using LLM \n",
    "query_engine = index.as_query_engine(\n",
    "    streaming=True,similarity_top_k=50)\n",
    "\n",
    "qq = \"Are there any papers about large language modeling? If so, could you group them by topic, give each paper a novelty score (0-10) and provide me the titles, links and a short summary?\"\n",
    "\n",
    "streaming_response = query_engine.query(qq)\n",
    "streaming_response.print_response_stream()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
