{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import transformer classes for generaiton\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "# Import torch for datatype attributes \n",
    "import torch\n",
    "import os\n",
    "auth_token = os.getenv(\"AUTH_TOKEN\")\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    print (\"There might be a problem!\")\n",
    "    \n",
    "# Define variable to hold llama2 weights naming \n",
    "name = \"meta-llama/Llama-2-7b-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(name, \n",
    "    cache_dir='./model/', return_token_type_ids=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = AutoModelForCausalLM.from_pretrained(name, \n",
    "    cache_dir='./model/',  torch_dtype=torch.float16, \n",
    "    rope_scaling={\"type\": \"dynamic\", \"factor\": 2}, load_in_8bit=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a test, generate a sentence\n",
    "\n",
    "# Setup a prompt \n",
    "prompt = \"### User:Which pizza toppings are generally not allowed by Italians? \\\n",
    "          ### Assistant:\"\n",
    "          \n",
    "# Pass the prompt to the tokenizer\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Setup the text streamer \n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "# Actually run the thing\n",
    "output = model.generate(**inputs, streamer=streamer, \n",
    "                        use_cache=True, max_new_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the prompt wrapper...but for llama index\n",
    "from llama_index.prompts.prompts import SimpleInputPrompt\n",
    "# Create a system prompt \n",
    "system_prompt = \"\"\"[INST] <>\n",
    "You are a helpful, respectful and honest assistant. Always answer as \n",
    "helpfully as possible, while being safe. Your answers should not include\n",
    "any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\n",
    "Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain \n",
    "why instead of answering something not correct. If you don't know the answer \n",
    "to a question, please don't share false information.\n",
    "\n",
    "Your goal is to provide answers relating to a set of research papers.<>\n",
    "\"\"\"\n",
    "# Throw together the query wrapper\n",
    "query_wrapper_prompt = SimpleInputPrompt(\"{query_str} [/INST]\",skip_on_failure=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the query prompt\n",
    "query_wrapper_prompt.format(query_str='hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the llama index HF Wrapper\n",
    "from llama_index.llms import HuggingFaceLLM\n",
    "# Create a HF LLM using the llama index wrapper \n",
    "llm = HuggingFaceLLM(context_window=4096,\n",
    "                    max_new_tokens=256,\n",
    "                    system_prompt=system_prompt,\n",
    "                    query_wrapper_prompt=query_wrapper_prompt,\n",
    "                    model=model,\n",
    "                    tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in embeddings wrapper\n",
    "from llama_index.embeddings import LangchainEmbedding\n",
    "# Bring in HF embeddings - need these to represent document chunks\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and dl embeddings instance  \n",
    "embeddings=LangchainEmbedding(\n",
    "    HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in stuff to change service context\n",
    "from llama_index import set_global_service_context\n",
    "from llama_index import ServiceContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new service context instance\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    chunk_size=1024,\n",
    "    llm=llm,\n",
    "    embed_model=embeddings\n",
    ")\n",
    "# And set the service context\n",
    "set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import feedparser\n",
    "import os\n",
    "\n",
    "#url = \"https://connect.biorxiv.org/biorxiv_xml.php?subject=developmental_biology\"\n",
    "\n",
    "url = \"http://arxiv.org/rss/cs.CL\"\n",
    "\n",
    "# Make a GET request to fetch the content of the page\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the content as XML using feedparser\n",
    "feed = feedparser.parse(response.content)\n",
    "\n",
    "# Now, 'feed' will contain the parsed RSS feed data\n",
    "# You can access elements like title, link, entries, etc.\n",
    "# Example:\n",
    "# print(\"Feed Title:\", feed.feed.title)\n",
    "# print(\"Feed Link:\", feed.feed.link)\n",
    "\n",
    "# # Access individual entries\n",
    "# for entry in feed.entries:\n",
    "#     print(\"\\nTitle:\", entry.title)\n",
    "#     print(\"Link:\", entry.link)\n",
    "#     print(\"Summary:\", entry.summary)\n",
    "\n",
    "# Create the 'abstracts' folder if it doesn't exist\n",
    "if not os.path.exists('abstracts'):\n",
    "    os.makedirs('abstracts')\n",
    "\n",
    "# Access individual entries and save them as separate documents\n",
    "for i, entry in enumerate(feed.entries, start=1):\n",
    "    title = entry.title\n",
    "    summary = entry.summary\n",
    "\n",
    "    # Create a file path for the abstract\n",
    "    file_path = os.path.join('abstracts', f'abstract_{i}.txt')\n",
    "\n",
    "    # Write the title and summary to the file\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(f'Title: {title}\\n\\n')\n",
    "        file.write(f'Summary: {summary}\\n')\n",
    "\n",
    "print(\"All abstracts saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import deps to load documents \n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "# Load documents from a directory\n",
    "documents = SimpleDirectoryReader('abstracts').load_data()\n",
    "\n",
    "# Create an index from the documents\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup index query engine using LLM \n",
    "query_engine = index.as_query_engine(\n",
    "    streaming=True,similarity_top_k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_response = query_engine.query(\n",
    "    \"Are there papers about knoledge graphs?\"\n",
    ")\n",
    "streaming_response.print_response_stream()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
